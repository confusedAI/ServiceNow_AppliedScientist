{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "################################   Some of the limitations or tasks I could not complete in the below code:\n",
        "# 1. Although I am using accelerate package for model parallelism the code was test on singel T4 GPU available on GCP.\n",
        "# 2. For Task9: Adjust dependent layers is not completed.\n",
        "\n",
        "# 3. For Task11: Training is only done on 8K samples from squad['train']. Due to limited time availabilty in Google Colab the data was cut short\n",
        "#                but the training code will work for whole data also. Same goes for evaluation, done only on 1K samples.\n",
        "#                Implications: The new trained model wont see all contexts in the train data since context repeats a lot in SQuAD dataset with different questions.\n",
        "#\n",
        "# 4. The train data and validation data was stored on list rather than a numpy array. List are computationally expensive in terms of space reducing batch-size.\n",
        "\n",
        "# 5. For Task12: I used F1 score(%) as the evaluation metric which is token overlap between the predicted answer and the reference answer.\n",
        "#    Pitfall of using F1 score: F1 score is sensitive to exact matches between predicted and reference answers.\n",
        "#                               If the model provides an answer that is semantically correct but not exactly matched with the reference answer,\n",
        "#                               the F1 score will penalize it.\n",
        "\n",
        "# 6. Did not creat a separate conda environment for the code to be portable. Direclty used the google colab notebook which provides for pre-installed\n",
        "#    packages like hugging-face, pytorch, etc."
      ],
      "metadata": {
        "id": "1OT802tE4mkG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0pbqJeNU_jo",
        "outputId": "11b941b9-a191-45be-aa06-fcd6a86a97f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate # Load a model on multiple GPUs with device_map=\"auto\". Accelerate provides for model parallelism.\n",
        "!pip install datasets # To download SQuAD dataset\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TFR4tq62WCgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d948f7ba-3f8f-4964-f610-400abd4f2e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32000\n"
          ]
        }
      ],
      "source": [
        "# Task1: Import GPU version of google/flan-t5-small from Hugging-face library\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "print(\"Vocabulary size:\", tokenizer.vocab_size) # Vocab Size\n",
        "# Vocabulary and Tokenizer: T5 using SentencePiece-based tokenizer, and the vocabulary (32000 tokens) built using SentencePiece to incorporate multiple languages.\n",
        "\n",
        "# Load the language model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlMonp5BYh42",
        "outputId": "f7b7a76f-cb7f-4225-d914-48f3238fa7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this dynamic tapestry of life, every encounter held the potential to unravel new narratives, intertwining the threads of destiny in an ever-evolving urban landscape.\n"
          ]
        }
      ],
      "source": [
        "# Task2: Verify if the summarization task works.\n",
        "\n",
        "# Random 100 words text in english\n",
        "data = \"In the heart of a bustling city, skyscrapers towered over crowded streets. People hurriedly navigated the urban maze, each with a unique story to tell. Neon lights flickered, casting a vibrant glow on the pavement. The aroma of diverse cuisines wafted from street vendors, creating a sensory symphony. Amidst the chaos, a sense of energy and possibility permeated the air. Time seemed to dance between the relentless pace of progress and the timeless essence of human connection. In this dynamic tapestry of life, every encounter held the potential to unravel new narratives, intertwining the threads of destiny in an ever-evolving urban landscape.\"\n",
        "\n",
        "# Tokenize the input text - Max token id's after tokenization = 512\n",
        "token_ids = tokenizer(data, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(\"cuda\")\n",
        "\n",
        "# Summary token id's\n",
        "summ_ids = model.generate(token_ids, max_length=150, length_penalty=2.0, num_beams=3, early_stopping=True)\n",
        "# length_penalty is the parameter that controls length of the sequences in output summary. Effectively helping in summarizing.\n",
        "# If length_penalty > 1.0, we will have shorter sequences.\n",
        "# If length_penalty < 0.5, we will have longer sequences.\n",
        "\n",
        "# Generated summary of around 25 words.\n",
        "print(tokenizer.decode(summ_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5GFFm12AYkVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a9d35b-df23-47d7-c9e6-752a05002489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delhi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Task3: Verify if the Q&A task works.\n",
        "\n",
        "# Context, Question\n",
        "context = \"Delhi is the capital city of India.\"\n",
        "question = \"What is the capital of India?\"\n",
        "data = f\"context: {context} question: {question}\"\n",
        "\n",
        "# Tokenize the data\n",
        "token_ids = tokenizer(data, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "answer_ids = model.generate(token_ids)\n",
        "\n",
        "# Decode the answer\n",
        "answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nrCrLf8UYl2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711b35af-f939-452a-a769-219529500e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M'ai nom Shivam. Travail en oeuvre de ServiceNow.\n"
          ]
        }
      ],
      "source": [
        "# Task4: Verify if English to French transla'on task works\n",
        "\n",
        "# English text\n",
        "data = \"English to French: My name is Shivam. Working on ServiceNow assignment.\"\n",
        "\n",
        "# Tokenize the data\n",
        "token_ids = tokenizer(data, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "french_ids = model.generate(token_ids) # Tensor containing the generated sequence in french.\n",
        "print(tokenizer.decode(french_ids[0], skip_special_tokens=True)) # French Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zxa6OkQPl7jb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfec4f87-d58c-4441-d309-238f4738f25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: shared.weight, Dimension: torch.Size([32128, 512])\n",
            "Name: encoder.block.0.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.0.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.0.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.0.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Dimension: torch.Size([32, 6])\n",
            "Name: encoder.block.0.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.0.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.0.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.1.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.1.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.1.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.1.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.1.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.1.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.1.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.2.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.2.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.2.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.2.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.2.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.2.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.2.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.3.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.3.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.3.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.3.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.3.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.3.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.3.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.4.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.4.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.4.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.4.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.4.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.4.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.4.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.5.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.5.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.5.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.5.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.5.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.5.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.5.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.6.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.6.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.6.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.6.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.6.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.6.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.6.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.7.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.7.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.7.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: encoder.block.7.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: encoder.block.7.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: encoder.block.7.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: encoder.block.7.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: encoder.final_layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.0.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.0.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.0.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.0.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Dimension: torch.Size([32, 6])\n",
            "Name: decoder.block.0.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.0.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.0.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.0.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.0.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.0.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.0.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.0.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.1.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.1.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.1.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.1.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.1.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.1.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.1.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.1.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.1.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.1.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.1.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.1.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.2.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.2.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.2.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.2.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.2.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.2.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.2.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.2.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.2.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.2.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.2.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.2.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.3.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.3.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.3.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.3.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.3.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.3.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.3.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.3.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.3.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.3.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.3.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.3.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.4.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.4.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.4.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.4.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.4.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.4.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.4.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.4.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.4.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.4.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.4.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.4.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.5.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.5.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.5.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.5.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.5.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.5.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.5.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.5.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.5.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.5.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.5.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.5.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.6.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.6.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.6.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.6.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.6.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.6.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.6.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.6.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.6.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.6.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.6.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.6.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.7.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.7.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.7.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.7.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.7.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.7.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.7.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.7.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
            "Name: decoder.block.7.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
            "Name: decoder.block.7.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
            "Name: decoder.block.7.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
            "Name: decoder.block.7.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: decoder.final_layer_norm.weight, Dimension: torch.Size([512])\n",
            "Name: lm_head.weight, Dimension: torch.Size([32128, 512])\n"
          ]
        }
      ],
      "source": [
        "# Task5: Programmatically print the names of all the model layers and their dimensions\n",
        "\n",
        "# Name and dimension of all model layers\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Name: {name}, Dimension: {param.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zn54gzt1mJIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1fb926-bbcd-4bd9-f90d-d6daa8a00800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of parameters: 76961152\n"
          ]
        }
      ],
      "source": [
        "# Task6: Programmatically print the total number of parameters/weights in this model.\n",
        "\n",
        "# Number of parameters\n",
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"# of parameters: {params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2cduOLEynNXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a827559b-8e47-4d14-94ce-f6aa6e4591cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of final decoder layer: 512\n",
            "Tensor for final output layer:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Task7: Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros.\n",
        "model.decoder.final_layer_norm.weight.data = torch.zeros_like(model.decoder.final_layer_norm.weight.data)\n",
        "print(f\"Dimension of final decoder layer: {len(model.decoder.final_layer_norm.weight.data)}\")\n",
        "print(f\"Tensor for final output layer:\\n{model.decoder.final_layer_norm.weight.data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kuuOpApVn5Tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd97f7a8-03a8-4c28-dc9b-ff59bd55f651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "# Task8: Verify if the Q&A task works after resetting the weights of the above layer.\n",
        "\n",
        "data = f\"context: {context} question: {question}\"\n",
        "token_ids = tokenizer(data, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "answer_ids = model.generate(token_ids)\n",
        "answer = tokenizer.decode(answer_ids[0]) # Default max_length = 20\n",
        "print(answer)\n",
        "\n",
        "# It does not work after resetting all weights of final decoder layer to zero. It just prints token corresponding to zero id which is <pad>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IeICfQnNosAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb7292e-3cb5-48a9-88e1-58ec11b30337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Output Dimension for final decoder layer: (128,)\n"
          ]
        }
      ],
      "source": [
        "# Task9: Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension\n",
        "\n",
        "new_dimensions = (model.config.d_model // 4,)  # Dimensionality of hidden states(512) / 4 = 128\n",
        "print(\"New Output Dimension for final decoder layer:\", new_dimensions)\n",
        "\n",
        "# New tensor with smaller dimensions\n",
        "small_dimension = torch.randn(new_dimensions) # Tensor with smaller dimensions\n",
        "small_dimension = small_dimension.repeat(model.decoder.final_layer_norm.weight.data.shape[0] // small_dimension.shape[0])\n",
        "\n",
        "# Update the model's weights\n",
        "model.decoder.final_layer_norm.weight.data = small_dimension\n",
        "\n",
        "# Adjust dependent layers --------------------------------------------- Remaining\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VtL46-LyqPBZ"
      },
      "outputs": [],
      "source": [
        "# Task 10: Reload the original google/flan-t5-small model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Aa2w89Qstrkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d6c49a-6265-4af2-cb82-70e5d58c6ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Size: 8000, Validation Data Size: 2000\n"
          ]
        }
      ],
      "source": [
        "# Task11: Train the model for a Q&A task\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AdamW\n",
        "from datasets import load_dataset, load_metric\n",
        "from tqdm import tqdm\n",
        "\n",
        "squad_dataset = load_dataset(\"squad\") # Load SQuAD dataset\n",
        "# Number of samples in the training set: 87599\n",
        "# Number of samples in the validation set: 10570\n",
        "\n",
        "Data = squad_dataset[\"train\"].select([i for i in range(10000)]) # Using only 10K samples from train set\n",
        "Full_train_data = []\n",
        "task_prefix = \"Question Answering\" # task prefix/trigger word\n",
        "\n",
        "# Data into Question, Context and answer triplets -> Tokenize to input_ids and attention mask with max_length = 256\n",
        "for inst in Data:\n",
        "  context = inst[\"context\"]\n",
        "  question = inst[\"question\"]\n",
        "  answer = inst[\"answers\"][\"text\"]\n",
        "\n",
        "  data = f\"{task_prefix}: context: {context} question: {question} answer: {answer}\"\n",
        "  token_ids = tokenizer(data, return_tensors=\"pt\", max_length=256, truncation=True, padding='max_length')\n",
        "  Full_train_data.append({'input_ids': token_ids['input_ids'].flatten(),'attention_mask': token_ids['attention_mask'].flatten()})\n",
        "\n",
        "# Full data split in train(80%) and validation(20%)\n",
        "train_prepared_data = Full_train_data[:int(0.8 * len(Full_train_data))]\n",
        "val_prepared_data = Full_train_data[int(0.8 * len(Full_train_data)):]\n",
        "print(f\"Train Data Size: {len(train_prepared_data)}, Validation Data Size: {len(val_prepared_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "o-ySgfnFkWj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fb3c63-b022-4276-ad3b-aaf44e6c5f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/5: 100%|██████████| 1000/1000 [05:32<00:00,  3.01it/s]\n",
            "Validation Epoch 1/5: 100%|██████████| 250/250 [00:26<00:00,  9.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.5570664405822754, Mean Validation Loss: 0.4405475752800703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 1000/1000 [05:31<00:00,  3.01it/s]\n",
            "Validation Epoch 2/5: 100%|██████████| 250/250 [00:26<00:00,  9.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.10793539881706238, Mean Validation Loss: 0.022706895373761655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 1000/1000 [05:34<00:00,  2.99it/s]\n",
            "Validation Epoch 3/5: 100%|██████████| 250/250 [00:27<00:00,  9.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.04521865397691727, Mean Validation Loss: 0.006208317199023441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 1000/1000 [05:34<00:00,  2.99it/s]\n",
            "Validation Epoch 4/5: 100%|██████████| 250/250 [00:26<00:00,  9.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.04251306131482124, Mean Validation Loss: 0.002639464108389802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 1000/1000 [05:34<00:00,  2.99it/s]\n",
            "Validation Epoch 5/5: 100%|██████████| 250/250 [00:26<00:00,  9.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.028646066784858704, Mean Validation Loss: 0.001378522665530909\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizer-flan-t5-small-squad/tokenizer_config.json',\n",
              " 'tokenizer-flan-t5-small-squad/special_tokens_map.json',\n",
              " 'tokenizer-flan-t5-small-squad/spiece.model',\n",
              " 'tokenizer-flan-t5-small-squad/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_loader = DataLoader(train_prepared_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_prepared_data, batch_size=8, shuffle=False)\n",
        "epochs = 5\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "# Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for val_batch in tqdm(val_loader, desc=f'Validation Epoch {epoch + 1}/{epochs}'):\n",
        "        val_inputs = val_batch['input_ids'].to(device)\n",
        "        val_attention_mask = val_batch['attention_mask'].to(device)\n",
        "        val_outputs = model(input_ids=val_inputs, attention_mask=val_attention_mask, labels=val_inputs)\n",
        "        val_loss += val_outputs.loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {loss.item()}, Mean Validation Loss: {val_loss / len(val_loader)}')\n",
        "\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"model-flan-t5-small-squad\")\n",
        "tokenizer.save_pretrained(\"tokenizer-flan-t5-small-squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-qU4Qd0xY37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a121f8a0-d1fb-4bbf-a127-544529e8bd7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/squad/squad.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Task 12: Evaluate the quality of the model\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"tokenizer-flan-t5-small-squad\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"model-flan-t5-small-squad\", device_map=\"auto\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "f1_metric = load_metric(\"squad\")\n",
        "cumulative_F1 = 0.0\n",
        "val_dataSize = 0\n",
        "\n",
        "for inst in squad_dataset[\"validation\"].select([i for i in range(1000)]): # Using only 1K samples from train set:\n",
        "\n",
        "    data = f\"{task_prefix}: context: {inst['context']} question: {inst['question']}\"\n",
        "    token_ids = tokenizer(data, return_tensors=\"pt\", max_length=256, truncation=True, padding='max_length')\n",
        "    inputs = {key: value.to(device) for key, value in token_ids.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        answer_ids = model.generate(**inputs, max_length=256, num_beams=1)\n",
        "\n",
        "    # Get the prediction text and original answer in right format to be used f1_metric function\n",
        "    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
        "    predictions = [{'prediction_text': answer, 'id': inst['id']}]\n",
        "    references = [{'answers': {'answer_start': inst['answers']['answer_start'], 'text': inst['answers']['text']}, 'id': inst['id']}]\n",
        "\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=references)\n",
        "    cumulative_F1 += f1['f1']\n",
        "    val_dataSize += 1\n",
        "\n",
        "print(f\"Mean F1 Score in %: {cumulative_F1 / val_dataSize}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4COo6C-DN_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}